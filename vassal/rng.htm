<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="author" content="">
<link rel="shortcut icon" href="/favicon.ico">
<title>VASL - Frequently Asked Questions</title>
<link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
<link href="/css/vasl_styles.css" rel="stylesheet">
</head>
<body>
<div id="navbar"></div>
<div class="container-fluid">
  <div class="row">
    <div class="col-md-2">
      <table width="100%" border="0" cellspacing="10" cellpadding="0">
        <tr>
          <td class="quoteHead" align="center">Questions on VASL 6.1?</td>
        </tr>
        <tr>
          <td align="center" class="quoteSource">Send questions to <br><a href="mailto:vasl@vasl.info">Sam Tyson</a></td>
        </tr>
      </table>
    </div>
    <div class="main-content col-md-10">

      <h2>How random is VASSAL's dice-roller?</h2>

      <p><a href="mailto:spb@meshuggeneh.net">Stephen P. Berry</a> has done some
      statistical analysis on a sample of dice rolls generated by the VASSAL
      engine.&nbsp; The short answer is that the results are indistinguishable
      from a random sample as far as any honest player would be able to tell.&nbsp;
      A maniacally motivated person might be able to guess the generator's seeding
      algorithm and therefore be able to make statistical predictions on future
      dice rolls based on the history of past dice rolls.&nbsp; However, I can
      recommend some much easier methods of cheating if that's what you're interested
      in.&nbsp; Here is the analysis:<u></u>

      <p><u>Distribution of rolls</u>
       From a test set of just over 35,000 2d6 rolls from the dicebot.&nbsp;
      The distribution looks pretty good:

      <p><tt>&nbsp;&nbsp; DR Count Expected</tt><br/><tt></tt><br/>

      <p><tt>&nbsp;&nbsp;&nbsp;&nbsp; 2&nbsp; 525&nbsp;&nbsp; 487.4</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; 943&nbsp;&nbsp; 974.9</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 4 1484&nbsp; 1462.3</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 5 1864&nbsp; 1949.8</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 6 2418&nbsp; 2437.2</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 7 2934&nbsp; 2924.7</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 8 2454&nbsp; 2437.2</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 9 1948&nbsp; 1949.8</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp; 10 1490&nbsp; 1462.3</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp; 11 1005&nbsp;&nbsp; 974.9</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp; 12&nbsp; 483&nbsp;&nbsp; 487.4</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      -------</tt><br/>
       <tt>&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17548</tt><br/>

      <p>Doing a simple chi-square hypothesis test, we get a chi-square of 9.82,
      which tells us that we do not reject the null hypothesis that <b>the data
      conforms to the random distribution at a significance level of 0.001</b>
      .The numbers work out even better if you just look at 1d6 instead of 2d6.
      So ...this tells us that the PRNG is spitting out numbers that are pretty
      uniform on {1, 2, 3, 4, 5, 6}.

      <p><u>Correlation between rolls</u>
       To test correlation between rolls, a count is kept for each pair {d[n
      - 1], d[n]} for n from 2 to the number of total dr.&nbsp; The average is
      figured as ((1/6)^2) * (n - 1), and a chi-square computed.&nbsp; The results
      for the 85450 1d6 I have data on are fine:

      <p>&nbsp;<tt> d[n] d[n-1] Count</tt><br/><tt></tt><br/>

      <p><tt>&nbsp; 1 1 2402</tt><br/>
       <tt>&nbsp; 1 2 2334</tt><br/>
       <tt>&nbsp; 1 3 2387</tt><br/>
       <tt>&nbsp; 1 4 2306</tt><br/>
       <tt>&nbsp; 1 5 2361</tt><br/>
       <tt>&nbsp; 1 6 2428</tt><br/>
       <tt>&nbsp; 2 1 2378</tt><br/>
       <tt>&nbsp; 2 2 2334</tt><br/>
       <tt>&nbsp; 2 3 2427</tt><br/>
       <tt>&nbsp; 2 4 2343</tt><br/>
       <tt>&nbsp; 2 5 2381</tt><br/>
       <tt>&nbsp; 2 6 2352</tt><br/>
       <tt>&nbsp; 3 1 2432</tt><br/>
       <tt>&nbsp; 3 2 2325</tt><br/>
       <tt>&nbsp; 3 3 2329</tt><br/>
       <tt>&nbsp; 3 4 2347</tt><br/>
       <tt>&nbsp; 3 5 2404</tt><br/>
       <tt>&nbsp; 3 6 2429</tt><br/>
       <tt>&nbsp; 4 1 2309</tt><br/>
       <tt>&nbsp; 4 2 2390</tt><br/>
       <tt>&nbsp; 4 3 2337</tt><br/>
       <tt>&nbsp; 4 4 2295</tt><br/>
       <tt>&nbsp; 4 5 2350</tt><br/>
       <tt>&nbsp; 4 6 2301</tt><br/>
       <tt>&nbsp; 5 1 2347</tt><br/>
       <tt>&nbsp; 5 2 2475</tt><br/>
       <tt>&nbsp; 5 3 2390</tt><br/>
       <tt>&nbsp; 5 4 2302</tt><br/>
       <tt>&nbsp; 5 5 2547</tt><br/>
       <tt>&nbsp; 5 6 2424</tt><br/>
       <tt>&nbsp; 6 1 2351</tt><br/>
       <tt>&nbsp; 6 2 2357</tt><br/>
       <tt>&nbsp; 6 3 2396</tt><br/>
       <tt>&nbsp; 6 4 2388</tt><br/>
       <tt>&nbsp; 6 5 2442</tt><br/>
       <tt>&nbsp; 6 6 2349</tt><br/>

      <p><tt>Expected value for each count:&nbsp; 2373.58333333333</tt><br/>
       <tt>Chi-square:&nbsp; 43.0407260471158</tt><br/>
       <tt>For df=35:&nbsp; p=0.01 p=0.005 p=0.001</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      49.80 57.34 66.62</tt><br/>

      <p>...so <b>there's no evidence there that the distribution of pairs of
      successive dr are not independent.</b><b></b>

      <p>Another test of correlations is popularly called the coupon collector's
      test. For the current audience, instead of coupons we might think of baseball
      cards or any other widget that comes in sets of which you want to `collect
      all n', for some sufficiently marketable value of n.

      <p>The idea is that we walk through our data, starting with an empty set,
      then add the elements of our data to the set, one by one, until we have
      acomplete set of one of each type (not counting duplicates).&nbsp; So for
      my dataset of 85450 1d6, we start with a set of zero 1s, zero 2s, and so
      on.&nbsp; Each time we see a new dr, we add it to our set.&nbsp; We keep
      track of how long it
       takes us to get a complete set of the numbers in [1, 6], then re-start
      from an empty set (selling the completed set on eBay, complete with photocopies
      of a couple other dr) and do it again.

      <p>After counting up the length between complete sets (i.e., the number
      of dr we had look through before we got one of each possible dr) we can
      do a trust ol' chi-square on the resulting data, where the probability
      for each length will be:

      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      d!
       &nbsp;<tt>p(r) = ----- * S((r - 1), (d - 1)) , d &lt;= r &lt; t</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d^r</tt><br/><tt></tt><br/>

      <p><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      d!</tt><br/>
       <tt>&nbsp;p(t) = 1 - ----------- * S((t - 1), d)</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      d^(t - 1)</tt><br/>

      <p>...where:&nbsp; r is the length in question;&nbsp; d is the number of
      items in a complete set;&nbsp; S(n,m) is the Stirling number of the second
      kind (which is the number of ways you can partition a set of n elements
      into m disjoint, nonempty subsets;&nbsp; and t the upper bound of the lengths
      we're going to consider (so if we were interested in lengths from 1 to
      10, t would be 11. we'll choose t such that the expected value for the
      highest r will be around 1.&nbsp; if this doesn't make sense, just peek
      down at the data and it'll probably be more clear).

      <p>We multiply these values for p(r) with the total number of lengths we
      measure to obtain an expected value for each length, then do our chi-square.

      <p>The data once again looks good:

      <p>&nbsp;<tt>Length Count Expected Value</tt><br/><tt></tt><br/>

      <p><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6&nbsp; 85&nbsp;&nbsp; 89.75309</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7 223&nbsp; 224.38272</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8 372&nbsp; 349.03978</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9 420&nbsp; 436.29973</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 10 475&nbsp; 481.38403</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 11 477&nbsp; 490.83719</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 12 490&nbsp; 474.63947</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 13 477&nbsp; 442.26326</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 14 401&nbsp; 401.22929</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 15 377&nbsp; 356.91274</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 16 293&nbsp; 312.85229</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 17 247&nbsp; 271.18876</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 18 235&nbsp; 233.07425</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 19 185&nbsp; 199.00003</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 20 174&nbsp; 169.03892</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 21 162&nbsp; 143.01511</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 22 104&nbsp; 120.61831</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 23&nbsp; 99&nbsp; 101.47771</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 24&nbsp; 75&nbsp;&nbsp; 85.20793</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 25&nbsp; 68&nbsp;&nbsp; 71.43616</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 26&nbsp; 50&nbsp;&nbsp; 59.81688</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 27&nbsp; 57&nbsp;&nbsp; 50.03876</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 28&nbsp; 54&nbsp;&nbsp; 41.82664</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 29&nbsp; 24&nbsp;&nbsp; 34.94069</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 30&nbsp; 39&nbsp;&nbsp; 29.17404</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 31&nbsp; 23&nbsp;&nbsp; 24.34958</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 32&nbsp; 24&nbsp;&nbsp; 20.31657</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 33&nbsp; 21&nbsp;&nbsp; 16.94732</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 34&nbsp; 13&nbsp;&nbsp; 14.13400</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 35&nbsp; 11&nbsp;&nbsp; 11.78582</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 36&nbsp; 10&nbsp;&nbsp;&nbsp; 9.82651</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 37&nbsp; 11&nbsp;&nbsp;&nbsp; 8.19208</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 38&nbsp;&nbsp; 7&nbsp;&nbsp;&nbsp; 6.82895</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 39&nbsp;&nbsp; 9&nbsp;&nbsp;&nbsp; 5.69227</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 40&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 4.74455</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 41&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; 3.95445</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 42&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 3.29581</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 43&nbsp;&nbsp; 3&nbsp;&nbsp;&nbsp; 2.74680</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 44&nbsp;&nbsp; 2&nbsp;&nbsp;&nbsp; 2.28920</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 45&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; 1.90779</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 46&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; 1.58991</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 47&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 1.32499</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 48&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp; 1.10419</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp; 49&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 0.92019</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp; =50&nbsp;&nbsp; 5&nbsp;&nbsp;&nbsp; 4.60124</tt><br/>
       <tt>&nbsp;</tt><br/>
       <tt>Chi-square:&nbsp;&nbsp;&nbsp; 41.73981</tt><br/>
       <tt>For df=43:&nbsp; p=0.01 p=0.005 p=0.001</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      59.30 67.46 77.42</tt><br/>

      <p>So...once again we aren't forced to reject the null hypothesis;&nbsp;
      <b>there's no evidence that the dice are biased in a statistically significant
      way.</b>

      <p>There's a more general test for this sort of thing, called a `birthday
      spacings' test (e.g., in Marsaglia's DIEHARD).

      <p>Imagine you have an arbitrary calendar of d days.&nbsp; Select b birthdays
      out of that calendar, and list them in nondescending order.&nbsp; Make
      a list of the distances between the birthdays, then sort them into nondescending
      order.&nbsp; Go through the list of distances, and count the number of
      times a distance is the same as the one that preceeded it (i.e., for distances
      s[0, 1, ...], count the times s[i] = s[i - 1]).&nbsp; Call this number
      n.&nbsp; Assuming that the birthdays were selected at random, then n should
      be Poisson distributed with a lambda of (b^3)/(4d).&nbsp; According to
      Marsaglia, experimentation suggests that you need d >= 2^18.

      <p>What are we doing here?&nbsp; We're just trying to come up with a general
      model that lets us figure out what an expected distribution for the differences
      between dr is.&nbsp; The birthday spacings test gives us such a model.&nbsp;
      Now we need to find a way to convert our dr into birthdays in a calendar
      (with lots and lots of days...specifically, at least 2^18
       days).

      <p>Okay.&nbsp; If we tried just mapping our die rolls directly to birthdays,
      we'd end up with a calendar with at most 6 days.&nbsp; That's somewhat
      less than our desired 2^18.&nbsp; So we figure ln(2^18)/ln(6) = 6.96.&nbsp;
      This tells us that 2^18 is roughly 6^6.96.&nbsp; What does this mean?&nbsp;
      Well, we're trying to figure out how many dr we need to string together
      to make a problem space roughly the same size as our desired calendar.&nbsp;
      Put in somewhat different terms, we can imagine a binary number as a bunch
      of coin tosses (each coin has two states---heads or tails---just like each
      bit has two states---0 or 1).&nbsp; What we're figuring out is more or
      less how many die rolls (things with six states) have as many possible
      combinations as 18 coin tosses (things with two states).

      <p>So if we do 18 coin tosses, there are 2^18 or 262144 possible results.
      By doing the ln(2^18)/ln(6) = 6.96, we're discovering that in order to
      get this many possible results out of die rolls, we need to roll about
      6.96 dice.&nbsp; Call it 7 dr, for a total of 6^7 = 279936 possible results---just
      over what we need.

      <p>This also tells us that when we're looking at the dice to generate birthdays
      in our calendar with 279936 days, we'll need to roll 7
       dice at a time.&nbsp; We'll also have to subtract one from each dr
      to insure we can generate numbers from 0 to 279935.&nbsp; So if we look
       at seven dr {1, 1, 1, 1, 1, 1, 2} we'd read that as (6^6 * 0) + (6^5
      * 0) + (6^4 * 0) + (6^3 * 0) + (6^2 * 0) + (6^1 * 0) + (6^0 * 1) = 1.&nbsp;
      And the dr {6, 6, 6, 6, 6, 6, 6} would be (6^6 * 5) + (6^5 * 5) + (6^4
      * 5) +
       (6^3 * 5) + (6^2 * 5) + (6^1 * 5) + (6^0 * 5) = 279935.

      <p>Okay.&nbsp; This all defined, we can grab dr, seven at a time, and convert
      them into an integer in [0, 279935].&nbsp; We can then walk through our
      data starting from the first dr and compute the number of recurrent birthday
      distances (our value n above in the original description of the test).&nbsp;
      We'll choose a number of birthdays per test that will keep our lambda managable.&nbsp;
      With 130 birthdays per test, it works out to (130^3) / (4 * 6^7) or about
      1.96 .&nbsp; We plug this into the Poisson
       equation:

      <p><tt>&nbsp;&nbsp; (e^(-l))(l^x)</tt><br/>
       <tt>&nbsp; P(x) = ---------------</tt><br/>
       <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      x!</tt><br/>

      <p>...where:&nbsp; l is our lambda and x is the number of collisions we're
      interested in {0, 1, 2, 3...}.&nbsp; We'll actually decide on a maximum
      value
       for x based on the data (the number of collisions will always be fairly
      small, and the expected number will depend on the number of samples we
      take).&nbsp; We'll also use the expected number and the observed number
      to do a chi-square test to get a p value for the test.

      <p>So much for the theory.&nbsp; Propping the poncho lifter back on my
      F2 key, I generated another 50354 dr which, along with my original data
      total up to 85450 dr.

      <p>I knocked together some code to run through multiple tests on these
      data, grabbing 910 dr at a go.&nbsp; That's 130 birthdays per test (as
       determined above), and 7 dr per birthday.&nbsp; So we'll get a total
      of (85450 / 910) = 93.9 collision counts.&nbsp; Ignoring the partial data
      set,
       we'll go with 93 values for n.

      <p>Reading through the data, before doing anything else, a chi-square value
      for the birthdays is determined---just to insure that
       our data aren't `failing' before we go further.&nbsp; Then the rest
      of the mechanics of the birthday spacings test are run through.

      <p>So much for the code.&nbsp; After a little number crunching, we learn:
       &nbsp;

      <p>&nbsp;<tt>Duplicate&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Observed&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      Expected</tt><br/>
       <tt>&nbsp;Spacings&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Number&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      Number</tt><br/>
       <tt>&nbsp;--------&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ------&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      ------</tt><br/>
       <tt>&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      12&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13.073</tt><br/>
       <tt>&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      34&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 25.650</tt><br/>
       <tt>&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      16&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 25.163</tt><br/>
       <tt>&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      19&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 16.457</tt><br/>
       <tt>&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.072</tt><br/>
       <tt>&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.168</tt><br/>
       <tt>&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.036</tt><br/>
       <tt>&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.290</tt><br/>
       <tt>&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.071</tt><br/>
       <tt>&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.016</tt><br/><tt></tt><br/>

      <p><tt>Chi-square:&nbsp; 8.83976326165167</tt><br/>

      <p>This is pretty darn good.&nbsp; We're certainly not lead to reject H0
      at any meaningful significance level.&nbsp; It's also worth noting that
      the
       birthday spacings test is often failed by PRNGs[3], so this is heartening
      for the suitability of the underlying PRNG for use in the dicebot.

      <p>So this is a pretty strong indicator that <b>we don't have statistically significant correlations between arbitrary numbers
      output by the dice bot</b>.
    </div>
  </div>
</div>

<footer></footer>
<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.0/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
	$("#navbar").load("/include/navbar.htm", function() {
    $("ul.navbar-nav li.faq").addClass("active");
	});
	$("footer").load("/include/copyright.htm");
	$("<script>").load("/include/tracking.js").appendTo("head");
});
</script>
</body>
</html>